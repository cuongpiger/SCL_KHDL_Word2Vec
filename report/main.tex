\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage[utf8]{vietnam}
\usepackage{tipa}
\usepackage{float}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  literate=%
    {đ}{{\dj}}1
    {â}{{\^a}}1
    {ă}{{\u{a}}}1
    {ê}{{\^e}}1
    {ô}{{\^o}}1
    {ơ}{{\ohorn}}1
    {ư}{{\uhorn}}1
    {á}{{\'a}}1
    {à}{{\`a}}1
    {ả}{\h{a}}1
    {ã}{{\~a}}1
    {ạ}{\textsubdot{a}}1
    {ấ}{\'{\^a}}1
    {ầ}{\`{\^a}}1
    {ẩ}{\h{\^a}}1
    {ẫ}{\~{\^a}}1
    {ậ}{\textsubdot{\^a}}1
    {ắ}{\'{\u{a}}}1
    {ằ}{\`{\u{a}}}1
    {ẳ}{\h{\u{a}}}1
    {ẵ}{\~{\u{a}}}1
    {ặ}{\textsubdot{\u{a}}}1
    {é}{{\'e}}1
    {è}{{\`e}}1
    {ẻ}{\h{e}}1
    {ẽ}{{\~e}}1
    {ẹ}{\textsubdot{e}}1
    {ế}{\'{\^e}}1
    {ề}{\`{\^e}}1
    {ể}{\h{\^e}}1
    {ễ}{\~{\^e}}1
    {ệ}{\textsubdot{\^{e}}}1
    {í}{{\'i}}1
    {ì}{{\`i}}1
    {ỉ}{\h{i}}1
    {ĩ}{{\~i}}1
    {ị}{\textsubdot{i}}1
    {ó}{{\'o}}1
    {ò}{{\`o}}1
    {ỏ}{\h{o}}1
    {õ}{{\~o}}1
    {ọ}{\textsubdot{o}}1
    {ố}{\'{\^o}}1
    {ồ}{\`{\^o}}1
    {ổ}{\h{\^o}}1
    {ỗ}{\~{\^o}}1
    {ộ}{\textsubdot{\^o}}1
    {ớ}{\'{\ohorn}}1
    {ờ}{\`{\ohorn}}1
    {ở}{\h{\ohorn}}1
    {ỡ}{\~{\ohorn}}1
    {ợ}{\textsubdot{\ohorn}}1
    {ú}{{\'u}}1
    {ù}{{\`u}}1
    {ủ}{\h{u}}1
    {ũ}{{\~u}}1
    {ụ}{\textsubdot{u}}1
    {ứ}{\'{\uhorn}}1
    {ừ}{\`{\uhorn}}1
    {ử}{\h{\uhorn}}1
    {ữ}{\~{\uhorn}}1
    {ự}{\textsubdot{\uhorn}}1
    {ý}{{\'y}}1
    {ỳ}{{\`y}}1
    {ỷ}{\h{y}}1
    {ỹ}{{\~y}}1
    {ỵ}{\textsubdot{y}}1
    {Đ}{{\DJ}}1
    {Â}{{\^A}}1
    {Ă}{{\u{A}}}1
    {Ê}{{\^E}}1
    {Ô}{{\^O}}1
    {Ơ}{{\OHORN}}1
    {Ư}{{\UHORN}}1
    {Á}{{\'A}}1
    {À}{{\`A}}1
    {Ả}{\h{A}}1
    {Ã}{{\~A}}1
    {Ạ}{\textsubdot{A}}1
    {Ấ}{\'{\^A}}1
    {Ầ}{\`{\^A}}1
    {Ẩ}{\h{\^A}}1
    {Ẫ}{\~{\^A}}1
    {Ậ}{\textsubdot{\^A}}1
    {Ắ}{\'{\u{A}}}1
    {Ằ}{\`{\u{A}}}1
    {Ẳ}{\h{\u{A}}}1
    {Ẵ}{\~{\u{A}}}1
    {Ặ}{\textsubdot{\u{A}}}1
    {É}{{\'E}}1
    {È}{{\`E}}1
    {Ẻ}{\h{E}}1
    {Ẽ}{{\~E}}1
    {Ẹ}{\textsubdot{E}}1
    {Ế}{\'{\^E}}1
    {Ề}{\`{\^E}}1
    {Ể}{\h{\^E}}1
    {Ễ}{\~{\^E}}1
    {Ệ}{\textsubdot{\^{E}}}1
    {Í}{{\'I}}1
    {Ì}{{\`I}}1
    {Ỉ}{\h{I}}1
    {Ĩ}{{\~I}}1
    {Ị}{\textsubdot{I}}1
    {Ó}{{\'O}}1
    {Ò}{{\`O}}1
    {Ỏ}{\h{O}}1
    {Õ}{{\~O}}1
    {Ọ}{\textsubdot{O}}1
    {Ố}{\'{\^O}}1
    {Ồ}{\`{\^O}}1
    {Ổ}{\h{\^O}}1
    {Ỗ}{\~{\^O}}1
    {Ộ}{\textsubdot{\^O}}1
    {Ớ}{\'{\OHORN}}1
    {Ờ}{\`{\OHORN}}1
    {Ở}{\h{\OHORN}}1
    {Ỡ}{\~{\OHORN}}1
    {Ợ}{\textsubdot{\OHORN}}1
    {Ú}{{\'U}}1
    {Ù}{{\`U}}1
    {Ủ}{\h{U}}1
    {Ũ}{{\~U}}1
    {Ụ}{\textsubdot{U}}1
    {Ứ}{\'{\UHORN}}1
    {Ừ}{\`{\UHORN}}1
    {Ử}{\h{\UHORN}}1
    {Ữ}{\~{\UHORN}}1
    {Ự}{\textsubdot{\UHORN}}1
    {Ý}{{\'Y}}1
    {Ỳ}{{\`Y}}1
    {Ỷ}{\h{Y}}1
    {Ỹ}{{\~Y}}1
    {Ỵ}{\textsubdot{Y}}1}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{indentfirst}
\lstset{basicstyle=\ttfamily, showstringspaces=false, commentstyle=\color{red}, keywordstyle=\color{blue}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
            {-2.5ex\@plus -1ex \@minus -.25ex}
            {1.25ex \@plus .25ex}
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand{\twiceIndent}{\hspace{\parindent}}
\newcommand{\leftIndent}{\setlength{\parindent}{0.6cm}}
\newcommand{\lleftIndent}{\setlength{\parindent}{1.2cm}}
\newcommand{\itemsizePaddingLeft}{\setlength{\itemindent}{0.6cm}}
\newcommand{\includeImage}[3]{
\begin{figure}[H]
  \centering
  \includegraphics[width=#1\textwidth]{images/#2.png}
  \def\temp{#3}\ifx\temp\empty\else\caption{#3}\fi
\end{figure}}

\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
  
  \center
  
  \textsc{\LARGE Đại học Khoa học tự nhiên}\\[1.5cm] 
  \textsc{\Large Môn học: Khoa Học Dữ Liệu}\\[0.5cm] 
  \textsc{\large Báo cáo đồ án cuối kỳ}\\[0.5cm] 
  
  \HRule \\[0.4cm]
  { \huge \bfseries Word2Vec}\\[0.4cm]
  \HRule \\[1.5cm]

  \begin{minipage}{0.4\textwidth}
  \begin{flushleft} \large
  \emph{\textbf{Giảng viên}}\\
  Nguyễn Ngọc Đức 
  \end{flushleft}
  \end{minipage}
  ~
  \begin{minipage}{0.4\textwidth}
  \begin{flushright}
    \large \emph{\\[3cm]\textbf{Sinh viên}} \\
    20424008 - Dương Mạnh Cường
  \end{flushright}
  \end{minipage}\\[2cm]

  \includegraphics{./images/logo_hcmus.png}\\[1cm]
  \vfill

  \today
  
  \end{titlepage}

\tableofcontents
\newpage

Neural network yêu cầu đầu vào ở dạng \textbf{numeric}. Cho nên, khi ta có dữ liệu dạng \textbf{text}, ta cần chuyển đổi chúng thành dữ liệu dạng numeric.\\

\indent Có nhiều phương pháp khác nhau để chuyển đổi dữ liệu dạng text sang numeric mà phổ biến nhất là:
\begin{itemize}
  \setlength{\itemindent}{0.6cm}
  \item Term frequency-inverse document frequency (TF-IDF).
  \item Bag of words (BOW).
\end{itemize}

\vskip 0.5cm
\indent Tuy nhiên, điểm yếu của hai phương pháp trên là chúng \textbf{không nắm bắt được ngữ nghĩa của từ}, nói cách khác là chúng không hiểu được ý nghĩa của từ.\\

\indent Có nhiều cách khắc phục nhược điểm này, mà một trong những cách đó là sử dụng \textbf{Word2Vec} bằng cách đại diện cho từng từ bằng một \textbf{vector} trong không gian $m$ chiều. Lúc này, các từ có nghĩa tương đồng nhau sẽ nằm gần nhau.

\includeImage{0.4}{01}{Các từ có ý nghĩa tương đồng nhau nằm gần nhau.}

\section{Word2Vec model}
Word2Vec là một trong những phương pháp \textbf{word embedding} được sử dụng phổ biến.\\

\indent Word embedding là cách ta biểu diễn các \textbf{word vector} trong không gian vector.\\

\indent Các word vector được tạo ra bởi Word2Vec model có khả năng nắm bắt được các \textbf{semantic} \textit{(ngữ nghĩa)} và \textbf{syntactic} \textit{(ý nghĩa cú pháp)} của từ.\\

\includeImage{1}{03}{Ví dụ về \textbf{semantic} và \textbf{syntactic}.}

\indent Ví dụ có câu: \textsl{"Archie used to live in New York, he then moved to Santa Clara. He loves apples and strawberries."}.\\

\indent Word2Vec model sẽ phát sinh các vector cho từng từ trong văn bản. Nếu chúng ta trực quan các vector này trong không gian vector tương ứng, chúng ta có thể thấy các từ tương tự nhau sẽ nằm gần nhau.

\includeImage{0.95}{02}{Các từ trong câu ví dụ được biểu diễn trong không gian vector}
\begin{tcolorbox}[grow to left by=-0.6cm]
  \textbf{Nhận xét}
  \begin{itemize}
    \item Ở đây các cặp từ như \textsl{apples} - \textsl{strawberries}, \textsl{New York} - \textsl{Santa Clara} có ý nghĩa tương đồng nhau nên nằm gần nhau.
  \end{itemize}
\end{tcolorbox}

\vskip 0.5cm
\indent Do đó, với Word2Vec model có thể học cách biểu diễn các vector giúp cho neural network có thể hiểu được ý nghĩa của từ tương ứng với vector đó.\\

\indent Và vì chúng ta có thể hiểu được semantic và syntactic của từ điều này giúp ta tận dụng các vector này vào các bài toán như \textbf{text summarization} \textit{(tóm tắt văn bản)}, \textbf{sentiment analysis} \textit{(phân tích tình cảm)}, \textbf{text generation} \textit{(tạo văn bản)},...\\

\indent Có hai cách để xây dựng Word2Vec model:
\begin{enumerate}
  \itemsizePaddingLeft
  \item \textbf{CBOW model}.
  \item \textbf{Skip-gram model}.
\end{enumerate}

\subsection{CBOW model}
Giả sử chúng ta có một neural network bao gồm: \textbf{một input layer}, \textbf{một hidden layer} và \textbf{một output layer}. Mục đích của network này là dự đoán ra \textbf{một từ} dựa vào \textbf{các từ xung quanh nó}. Từ mà chúng ta cố gắng dự đoán được gọi là \textbf{target word} và các từ xung quanh nó được gọi là \textbf{context word}.\\

\indent Vậy chúng ta cần bao nhiêu context word để dự đoán ra target word? Chúng ta sẽ sử dụng một \textbf{window} \textit{(cửa sổ)} có kích thước là $n$ để chọn các context word. Nếu $n = 2$ thì chúng ta sẽ sử dụng hai từ \textbf{phía trước} và \textbf{phía sau} của target word làm các context word.\\

\indent Xem xét câu sau: \textsl{"The Sun rises in the east."} với \textsl{rises} là target word. Nếu chúng ta xét kích thước của window là $2$ thì chúng ta sẽ lấy hai từ phía trước là \textsl{The}, \textsl{Sun} và hai từ phía sau là \textsl{in}, \textsl{the} của target word làm các context word.\\

\includeImage{0.7}{04}{Target word và context word trong CBOW model.}

\indent Lúc này input của network là các context word và output của network là target word. Và vì neural network chỉ chấp nhận input là numeric data nên chúng ta sẽ sử dụng kỹ thuật \textbf{one-hot encoding} để chuyển đổi các text data thành numeric data.

\includeImage{0.6}{05}{One-hot encoding cho text data.}

\indent Kiến trúc của CBOW model được thể hiện dưới hình sau. Ở đây các context word là: \textsl{the}, \textsl{sun}, \textsl{in} và \textsl{east} được dùng làm đầu vào cho network và dự đoán ra target word là \textsl{rises} ở đầu ra.

\includeImage{0.99}{06}{Kiến trúc của CBOW model 1.}

\indent Ở vài lần lặp đầu tiên, network không thể dự đoán target word một cách chính xác. Nhưng sau một loạt các vòng lặp bằng cách sử dụng \textbf{gradient descent}, các \textbf{weight} \textit{(trọng số)} của network được cập nhật và tìm ra được \textbf{optimal weight} \textit{(trọng số thích hợp)} để dự đoán ra target word một cách chính xác.

\vskip 0.5cm
\indent Vì chúng ta có một input layer, một hidden layer và một output layer, nên chúng ta sẽ có hai weight:
\begin{enumerate}
  \itemsizePaddingLeft
  \item Weight từ input layer đến hidden layer - $\boldsymbol{W}$.
  \item Weight từ hidden layer đến output layer - $\boldsymbol{W'}$.
\end{enumerate}

\indent Trong quá trình đào tạo network, các weight sẽ được cập nhật trong quá trình back propagation nhằm tìm ra optimal weight cho hai bộ $\boldsymbol{W}$ và $\boldsymbol{W'}$.

\vskip 0.5cm
\indent Về sau, weight set giữa input layer và hidden layer $\boldsymbol{W}$ được cập nhật và tối ưu tạo thành các vector đại diện cho các từ của input layer.\\

\indent Sau khi kết thúc quá trình đào tạo, chúng ta chỉ cần loại bỏ output layer và lấy ra weight set giữa input layer và hidden layer và gán chúng cho các từ tương ứng.\\

\indent Dưới đây là các vector tương ứng cho các từ của $\boldsymbol{W}$. Word embedding tương ứng cho từ \textsl{sun} là $\begin{bmatrix} 0.0 & 0.3 & 0.3 & 0.6 & 0.1 \\ \end{bmatrix}$.

$$\boldsymbol{W} = \text{ } \begin{matrix} the \\ sun \\ rises \\ in \\ east \end{matrix} \begin{bmatrix} 0.01 & 0.02 & 0.1 & 0.5 & 0.37 \\ 0.0 & 0.3 & 0.3 & 0.6 & 0.1 \\ 0.4 & 0.34 & 0.11 & 0.61 & 0.43 \\ 0.1 & 0.11 & 0.1 & 0.17 & 0.369 \\ 0.33 & 0.4 & 0.3 & 0.17 & 0.1 \\ \end{bmatrix}$$

\subsubsection{CBOW model với một context word}
CBOW model cần một số lượng context word $C$ nhất định để dự đoán target word. Ở phần này chúng ta sẽ xem xét trường hợp chỉ sử dụng duy nhất một context word, tức $C = 1$. Lúc này netword nhận vào một context word ở đầu vào và trả về một target word ở đầu ra.\\

\indent Trước tiên, chúng ta cần làm quen với một vài khái niệm. Tất cả các từ duy nhất nằm trong \textbf{corpus} \textit{(kho ngữ liệu)} của chúng ta được gọi là các \textbf{vocabulary} \textit{(từ vựng)}. Trong ví dụ trước đó của chúng ta, chúng ta có năm từ là: \textsl{the}, \textsl{sun}, \textsl{rises}, \textsl{in} và \textsl{east} - toàn bộ năm từ này chính là vocabulary trong corpus của chúng ta.\\

\indent Đặt $V$ là số lượng vocabulary và $N$ là số neuron của hidden layer. Vì neural network của chúng ta có ba layer, cụ thể:
\begin{itemize}
  \itemsizePaddingLeft
  \item Input layer được đại diện bởi $\boldsymbol{X} = \{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_3,...,\boldsymbol{x}_k,...,\boldsymbol{x}_V \}$. Khi chúng ta nói $\boldsymbol{x}_k$, tức ta đang đề cập đến từ thứ $k$ trong corpus của chúng ta.
  \item Hidden layer được đại diện bởi $\boldsymbol{H} = \{\boldsymbol{h}_1,\boldsymbol{h}_2,\boldsymbol{h}_3,...,\boldsymbol{h}_i,...,\boldsymbol{h}_N \}$. Khi chúng ta nói $\boldsymbol{h}_i$, tức ta đang đề cập đến neuron thứ $i$ trong hidden layer.
  \item Output layer được đại diện bởi $\boldsymbol{Y} = \{\boldsymbol{y}_1,\boldsymbol{y}_2,\boldsymbol{y}_3,...,\boldsymbol{y}_j,...,\boldsymbol{y}_V \}$. Khi chúng ta nói $\boldsymbol{y}_j$, tức ta đang đề cập đến từ thứ $j$ trong output layer.
\end{itemize}

\vskip 0.5cm
\indent Số chiều của $\boldsymbol{W}$ là $V \times N$, tức $\textit{số vocabulary} \times \textit{số neuron trong hidden layer}$. $W_{ki}$ đại diện cho một phần tử trong ma trận $\boldsymbol{W}$ giữa $\boldsymbol{x}_k$ trong input layer và $\boldsymbol{h}_i$ trong hidden layer.

\vskip 0.5cm
\indent Số chiều của $\boldsymbol{W'}$ là $N \times V$, tức $\textit{số neuron trong hidden layer} \times \textit{số vocabulary}$. $W'_{ij}$ đại diện cho một phần tử trong ma trận $\boldsymbol{W'}$ giữa $\boldsymbol{h}_i$ trong hidden layer và $\boldsymbol{y}_j$ trong output layer.
     
\includeImage{0.7}{07}{Kiến trúc của CBOW model 2.}

\paragraph{Forward propagation}
Để dự đoán target word từ các context word, chúng ta sẽ thực hiện quá trình forward propagation. Trước tiên, chúng ta nhân $\boldsymbol{X}$ cho $\boldsymbol{W}$.
$$\boldsymbol{H} = \boldsymbol{XW}^T$$

\indent Chúng ta biết rằng input là các one-hot vector, nên khi ta nhân $\boldsymbol{X}$ với $\boldsymbol{W}$ thì $\boldsymbol{H}$ lúc này đơn giản là:
$$\boldsymbol{H} = \boldsymbol{W}_{(k,.)}$$

\indent $\boldsymbol{W}_{(k,.)}$ về cơ bản là biểu diễn vector của input word. Đặt vector đại diện cho input word $w_I$ bằng $Z_{w_I}$. Lúc này phương trình trên có thể được viết thành.\\
\begin{equation}
  \boldsymbol{H} = Z_{w_I}
\end{equation}

\indent Bây giờ chúng ta đang ở hidden layer, tiếp theo chúng ta cần tính xác suất cho từng từ trong corpus để nó là một target word.\\

\indent Đặt $\boldsymbol{u}_j$ là điểm số để từ thứ $j$ trong corpus là target word - được tính bằng cách nhân $\boldsymbol{H}$ cho $\boldsymbol{W'}$. Và vì chúng ta đang tính toán điểm số cho từ $j$ trong corpus, nên ta chỉ nhân cột $j$ trong $\boldsymbol{W'}$ cho $\boldsymbol{H}$.
$$\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$$
\indent Lúc này, cột $j$ của $\boldsymbol{W'}_{ij}$ đại diện cho vector của từ $j$ trong corpus. Đặt vector đại diện cho từ thứ $j$ là $Z'_{w_j^T}$. Phương trình trên có thể được viết lại thành:
\begin{equation}
  \boldsymbol{u}_j = Z'_{w_j^T} \cdot \boldsymbol{H}
\end{equation}

\indent Thế phương trình $(1)$ vào phương trình $(2)$, ta được:
$$\boldsymbol{u}_j = Z'_{w_j^T} \cdot Z_{w_I}$$

\indent Việc tính toán dot product giữa $Z'_{w_j^T}$ và $Z_{w_I}$ nói cho chúng ta biết độ tương đồng giữa target word với các context word đầu vào. Cho nên nếu điểm số của từ $j$ trong corpus cao thì có nghĩa có sự tương đồng cao giữa các context word và target word $j$ này và ngược lại.\\

\indent Cuối cùng, ta sẽ áp dụng softmax activation để chuyển đổi các điểm số này sang xác suất:
\begin{equation}
  \boldsymbol{y}_j = \dfrac{\exp{(\boldsymbol{u}}_j)}{\sum_{j'=1}^V \exp{(\boldsymbol{u}'_j)}}
\end{equation}

\indent Lúc này, $\boldsymbol{y}_j$ cho chúng ta biết xác suất của từ $j$ là target word. Chúng ta sẽ tính xác suất cho tất cả các từ trong corpus và chọn từ mà có xác suất cao nhất là target word.\\

\indent Tiếp theo, chúng ta sẽ tính toán hàm loss. Đặt $\boldsymbol{y}_j^*$ là xác suất để từ $j$ chính xác là target word. Chúng ta cần tối đa xác suất này:
$$\max{\boldsymbol{y}_j^*}$$

\indent Và để giảm phạm vị khi tính hàm loss ta sẽ tính tối đa hàm log của xác suất.
$$\max{\log{(\boldsymbol{y}_j^*)}}$$

\indent Nhưng vì chúng ta sẽ áp dụng gradient descent để tìm ra optimal weight nên thay vì tối đa hóa ta sẽ tìm tối thiểu hóa công thức trên như sau:
$$\min (-\log{(\boldsymbol{y}_j^*)})$$

\indent Vậy lúc này loss function của chúng ta sẽ thành:
\begin{equation}
  \mathfrak{L} = -\log{(\boldsymbol{y}_j^*)}
\end{equation}

\indent Thế phương trình $(3)$ vào phương trình $(4)$ ta thu được:
\[\begin{aligned}\mathfrak{L} &= - \log{\left ( \dfrac{\exp{(u_j)}}{\sum_{j'=1}^V \exp{(u'_j)}} \right )} \\
   &= - \left ( \log{(\exp{(u_j)})} - \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )} \right ) \\ 
   &= -\log{(\exp{(u_j)})} + \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )} \\
   &= -u_j + \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )}\end{aligned}\]

\paragraph{Backward propagation}
Để tối thiểu hóa loss function, chúng ta sử dụng gradient descent. Chúng ta sẽ tính gradient của loss function để cập nhật weight.\\

\indent Chúng ta có hai weight set là $\boldsymbol{W}$ và $\boldsymbol{W'}$. Trong quá trình backward propagration, chúng ta có thể tiến hành cập nhật weight cho chúng. Cụ thể như sau:
$$\boldsymbol{W} = \boldsymbol{W} - \alpha \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}}$$ 
$$\boldsymbol{W'} = \boldsymbol{W'} - \alpha \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}}$$ 

\indent Trước tiên, trong quá trình forward propagation chúng ta có các phương trình sau:
$$\boldsymbol{H} = \boldsymbol{XW}^T$$
$$\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$$
$$\mathfrak{L} = -\boldsymbol{u}_j + \log{\left ( \sum_{j'=1}^V \exp{(\boldsymbol{u'}_j)} \right )}$$

\indent Trước tiên, chúng ta sẽ tính gradient của loss function cho $\boldsymbol{W'}$. Chúng ta không thể tính gradient của loss function đối với $\boldsymbol{W'}$ trực tiếp. Cho nên chúng ta phải áp dụng \textbf{chain rule} như sau:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}_{ij}} = \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} \cdot \dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}}$$

\indent Ta có đạo hàm của $\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j}$ như sau:
\begin{equation}
  \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} = \boldsymbol{e}_j
\end{equation}

\twiceIndent với $\boldsymbol{e}_j$ là sự khác biệt giữa actual word và predicted word.

\vskip 0.5cm
\indent Tiếp theo, chúng ta sẽ tính đạo hàm cho $\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}}$, và bởi vì chúng ta biết $\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$ nên:
$$\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}} = \boldsymbol{H}$$

\indent Như vậy, gradient của loss function đối với $\boldsymbol{W'}$ bằng:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}_{ij}} = \boldsymbol{e}_j \cdot \boldsymbol{H}$$

\indent Tiếp theo, chúng ta cần tính gradient của loss function đối với $\boldsymbol{W}$, và chúng ta cũng không thể tính gradient trực tiếp nên chúng ta phải áp dụng chain rule.
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}_{ki}} = \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} \cdot \dfrac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{W}_{ki}}$$

\indent Để tính đạo hàm của $\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i}$, chúng ta áp dụng chain rule:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} \cdot \dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{h}_i}$$

\indent Thế phương trình $(5)$ vào phương trình trên, ta được:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \boldsymbol{e}_j \cdot \dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{h}_i}$$

\indent Và vì chúng ta biết $\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$, tiếp tục thế vào phương trình trên chúng ta được:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \boldsymbol{e}_j \cdot \boldsymbol{W'}_{ij} = \mathfrak{L}\boldsymbol{H}^T$$

\indent Bây giờ chúng ta sẽ tính đạo hàm cho $\dfrac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{W}_{ki}}$. Và vì chúng ta biết $\boldsymbol{H} = \boldsymbol{XW}^T$ nên:
$$\dfrac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{W}_{ki}} = \boldsymbol{X}$$

\indent Cuối cùng, gradient của loss function đối với $\boldsymbol{W}$ là:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}_{ki}} = \boldsymbol{LH}^T \cdot \boldsymbol{X}$$

\indent Như vậy, các set weight $\boldsymbol{W}$ và $\boldsymbol{W'}$ của chúng ta lần lượt được cập nhật trong quá trình backward propagation như sau:
$$\boldsymbol{W} = \boldsymbol{W} - \alpha \boldsymbol{LH}^T \cdot \boldsymbol{X}$$
$$\boldsymbol{W'} = \boldsymbol{W'} - \alpha \boldsymbol{e}_j \cdot \boldsymbol{H}$$

\indent Chúng ta thực hiện cập nhật $\boldsymbol{W}$ và $\boldsymbol{W'}$ đến cuối quá trình đào tạo network, $\boldsymbol{W}$ của chúng ta lúc này đạt được trạng thái optimal weight, ta có thể lấy nó ra làm các word vector đại diện cho các vocabulary của chúng ta trong corpus.

\indent Dưới đây là mã nguồn Python để tìm kiếm các word vector.
\begin{lstlisting}[language=python]
def single_context_CBOW(x, label, W1, W2, loss):
  # forward propagation
  h = np.dot(W1.T, x)
  u = np.dot(W2.T, h)
  y_pred = softmax(u)

  # error giữa actual value và predicted value + tính backward propagation
  e = label - y_pred
  dW2 = np.outer(h, e)
  dW1 = np.outer(x, np.dot(W2.T, e))

  # cập nhật các weight set
  W1 = W1 - lr * dW1
  W2 = W2 - lr * dW2

  # loss function
  loss += -float(u[label] == 1) + np.log(np.sum(np.exp(u)))

  return W1, W2, loss
\end{lstlisting}

\subsubsection{CBOW model với multiple context words}
Bây giờ, chúng ta sẽ tìm hiểu cách CBOW model với multiple context words hoạt động. Kiến trúc của CBOW model với multiple context words như sau:

\includeImage{1}{08}{Kiến trúc của CBOW model với multiple context words.}

\indent Không có nhiều sự khác biệt giữa single context word và multiple context words. Sự khác biệt là với multiple context words, chúng ta lấy giá trị trung bình của tất cả các context word đầu vào. Cụ thể với $C$ context word $\boldsymbol{X}_1, \boldsymbol{X}_2, \hdots, \boldsymbol{X}_C$, chúng ta thực hiện:
$$\begin{aligned}\boldsymbol{H} &= \dfrac{(\boldsymbol{X}_1 + \boldsymbol{X}_2 + \hdots + \boldsymbol{X}_C)}{C} \boldsymbol{W}^T \\ &= \dfrac{1}{C}(\boldsymbol{X}_1 \boldsymbol{W}^T + \boldsymbol{X}_1 \boldsymbol{W}^T + \hdots + \boldsymbol{X}_C \boldsymbol{W}^T) \end{aligned}$$

\indent Tương tự như CBOW model với single context word, $\boldsymbol{X}_1 \boldsymbol{W}^T$ đại diện cho vector của input context word $w_1$. Chúng ta đại diện $Z_{w_1}$ cho input context word $w_1$, cho nên:
\begin{equation}\boldsymbol{H} = \dfrac{1}{C} (Z_{w_1} + Z_{w_2} + \hdots + Z_{w_C})\end{equation}

\indent Ở đây, $C$ đại diện cho số lượng context word, để tính toán giá trị của $\boldsymbol{u}_j$ sẽ tương tự như cách ta tính toán cho single context word.
\begin{equation}
  \boldsymbol{u}_j = Z'_{w_j^T} \cdot \boldsymbol{H}
\end{equation}

\indent Ở đây, $Z'_{w_j^T}$ là vector đại diện cho từ thứ $j$ trong corpus.\\

\indent Thế phương trình $(6)$ vào phương trình $(7)$, ta được:
$$\boldsymbol{u}_j = Z'_{w_j^T} \cdot \dfrac{1}{C} (Z_{w_1} + Z_{w_2} + \hdots + Z_{w_C})$$

\indent Tiếp theo, chúng ta sẽ tính toán cho loss function - nó tương tự như những gì chúng ta đã làm cho loss function ở single context word.
$$\mathfrak{L} = -u_j + \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )}$$

\indent Tiếp theo, là quá trình backward propagation. Cụ thể, quá trình cập nhật cho $\boldsymbol{W}$ sẽ như sau:
$$\boldsymbol{W} = \boldsymbol{W} - \alpha \mathfrak{L} \boldsymbol{H}^T \cdot \dfrac{(\boldsymbol{X}_1 + \boldsymbol{X}_2 + \hdots + \boldsymbol{X}_C)}{C}$$

\indent Đối với $\boldsymbol{W'}$ sẽ tương tự như single context word:
$$\boldsymbol{W'} = \boldsymbol{W'} - \alpha \boldsymbol{e}_j \cdot \boldsymbol{H}$$

\subsection{Skip-gram model}
Skip-gram model về cơ bản như là việc làm ngược lại CBOW model. Với Skip-gram model, chúng ta sẽ cố gắng dự đoán các context word dựa vào target word được cung cấp ở đầu vào. Lấy lại ví dụ ban đầu của CBOW model, chúng ta sẽ dùng target word là \textsl{rises} để dự đoán các context word là \textsl{the}, \textsl{sun}, \textsl{in} và \textsl{east}.

\includeImage{0.7}{04}{Target word và context word trong Skip-gram model.}

\indent Dưới đây là kiến trúc của Skip-gram model:
\includeImage{1}{09}{Kiến trúc của Skip-gram model 1.}

\indent Trong Skip-gram model chúng ta sẽ cố gắng dự đoán các context word dựa vào target word. Ở đây ta chỉ có một target word và sẽ dự đoán ra $C$ context word. Sau quá trình đào tạo chúng ta sẽ lấy weight set giữa input layer và hidden layer $\boldsymbol{W}$ làm vector đại diện cho các từ. Bây giờ chúng ta sẽ tìm hiểu quá trình forward propagation và backward propagation.

\subsubsection{Forward propagation}
Hình dưới đây là kiến trúc của Skip-gram model, chúng ta có một target word làm đầu vào $\boldsymbol{X}$ và trả về $C$ context word $\boldsymbol{Y}$ ở đầu ra.
\includeImage{1}{10}{Kiến trúc của Skip-gram model 2.}

\indent Tương tự như CBOW model, trong quá trình forward propagation, trước tiên chúng ta nhân $\boldsymbol{X}$ cho hidden layer weight $\boldsymbol{W}$.
$$\boldsymbol{H} = \boldsymbol{XW}^T$$

\indent Tương tự như CBOW model, chúng ta có thể viết lại như sau:
$$\boldsymbol{H} = Z_{w_I}$$

\twiceIndent ở đây, $Z_{w_I}$ là vector đại diện cho input word $w_I$.\\

\indent Tiếp theo, chúng ta cần tính $\boldsymbol{u}_j$, đây chính là điểm số dùng để đánh giá sự tương đồng giữa từ $j$ trong corpus và target word. Cũng giống với CBOW model, $\boldsymbol{u}_j$ được tính:
$$\begin{aligned}\boldsymbol{u}_j &= \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H} \\ 
  &= Z'_{w_j^T} \cdot \boldsymbol{H} \end{aligned}$$

\twiceIndent với $Z'_{w_j^T}$ là vector đại diện cho từ $j$.\\

\indent Nhưng không giống với CBOW model chúng ta chỉ cần dự đoán duy nhất một target word. Ở đây, chúng ta sẽ dự đoán tra $C$ context word. Nên để cho tường minh chúng ta có thể ghi lại phương trình trên thành:
$$\boldsymbol{u}_{c,j} = Z'_{w_j^T} \cdot \boldsymbol{H} \text{ với } c = 1, 2, 3, \hdots,C$$

\indent Vì $\boldsymbol{u}_{c,j}$ là điểm số đại diện cho từ thứ $j$ là context word $c$. Cho nên:
\begin{itemize}
  \itemsizePaddingLeft
  \item $\boldsymbol{u}_{1,j}$ là điểm số cho từ thứ $j$ sẽ là context word đầu tiên.
  \item $\boldsymbol{u}_{2,j}$ là điểm số cho từ thứ $j$ sẽ là context word thứ hai.
  \item $\boldsymbol{u}_{c,j}$ là điểm số cho từ thứ $j$ sẽ là context word thứ $c$.
\end{itemize}

\vskip 0.5cm
\indent Tiếp theo, chúng ta chuyển toàn bộ các điểm số trên thành xác suất bằng hàm softmax và đại diện bởi $\boldsymbol{y}_{c,j}$.

\begin{equation}
  \boldsymbol{y}_{c,j} = \dfrac{\exp{(\boldsymbol{u}_{c,j})}}{\sum_{j'=1}^V \exp{(\boldsymbol{u}_{j'})}}
\end{equation}

\indent Ở đây, $\boldsymbol{y}_{c,j}$ là xác suất cho từ thứ $j$ trong corpus sẽ là context word thứ $c$.\\

\indent Bây giờ chúng ta sẽ tìm hiểu cách tính hàm loss function. Đặt $\boldsymbol{y}^*_{c,j}$ là xác suất để từ được chọn đúng là context word. Chúng ta cần tối đa hóa xác suất này.
$$\max{\boldsymbol{y}^*_{c,j}}$$

\indent Và cũng tương tự như CBOW model, thay vì tối đa hóa hàm trên, ta sẽ đi tối thiểu hóa hàm $log$ và lấy âm.
$$\min{(-\log{(\boldsymbol{y}^*_{c,j})})}$$

\indent Thế phương trình $(8)$ vào phương trình trên ta được:
$$\mathfrak{L} = -\log{\dfrac{\exp{(\boldsymbol{u}_{c,j})}}{\sum_{j'=1}^V \exp{(\boldsymbol{u}_{j'})}}}$$

\indent Và vì chúng ta có $C$ context word, nên chúng ta sẽ lấy tích các xác suất này với nhau.
$$\begin{aligned} \mathfrak{L} &= -\log{\prod_{c=1}^{C} \dfrac{\exp{(\boldsymbol{u}_{c,j})}}{\sum_{j'=1}^V \exp{(\boldsymbol{u}_{j'})}}} \\ 
&= -\sum^{C}_{c=1} \boldsymbol{u}_{c,j} + C \cdot \log{\sum^V_{j'=1}} \exp{(\boldsymbol{u}_{j'})} \end{aligned}$$

\subsubsection{Backward propagation}
Để tối thiểu hóa loss function, chúng ta áp dụng thuật toán gradient descent. Trong quá trình backward propagation trên network, chúng ta tính toán gradient cho loss function và cập nhật cho weight.\\

\indent Trước tiên, chúng ta tính toán gradient cho $\boldsymbol{W'}$ bằng chain rule như sau:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}_{ij}} = \sum^C_{c=1} \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_{c,j}} \cdot \dfrac{\partial \boldsymbol{u}_{c,j}}{\partial \boldsymbol{W'}_{ij}}$$

\indent Tương tự như CBOW model, đạo hàm của $\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_{c,j}}$ chính là sự khác biệt giữa actual value và predicted value:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_{j}} = \boldsymbol{e}_{c,j}$$

\indent Tuy nhiên, do chúng ta có nhiều context word, nên chúng ta phải cộng tất cả các $\boldsymbol{e}_{c,j}$ này lại:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_{j}} = \sum^C_{c=1} \boldsymbol{e}_{c,j} = E$$

\indent Bây giờ, chúng ta sẽ tính đạo hàm cho $\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}}$, và bởi vì chúng ta biết $\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$, cho nên:

$$\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}} = \boldsymbol{H}$$

\indent Cho nên, gradient của loss function đối với $\boldsymbol{W'}$ như sau:
$$\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}} = E \cdot \boldsymbol{H}$$

\indent Bây giờ, chúng ta sẽ tính gradient của loss function đối với $\boldsymbol{W}$. Nó về cơ bản cũng tương tự như CBOW model.
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}_{ki}} = \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} \cdot \dfrac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{W}_{ki}}$$

\indent Cho nên gradient của loss function so với $\boldsymbol{W}$ và $\boldsymbol{W'}$ như sau:
$$\boldsymbol{W'} = \boldsymbol{W'} - \alpha \boldsymbol{E} \cdot \boldsymbol{H}$$
$$\boldsymbol{W} = \boldsymbol{W} - \alpha \boldsymbol{LH}^T \cdot \boldsymbol{X}$$

\indent Kết thúc quá trình đào tạo, chúng ta sẽ tiến hành cập nhật cho các weight set trên network. Sau cùng $\boldsymbol{W}$ đạt trạng thái optimal weight và trở thành word vector cho vocabulary trong corpus.

\section{Các chiến lược cải thiện hiệu suất}
Để tiến hành cải thiện hiệu suất của quá trình đào tạo model, người ta sẽ sử dụng kèm theo một số kỹ thuật như:
\begin{itemize}
  \itemsizePaddingLeft
  \item Sử dụng \textbf{Hierarchical softmax} thay vì softmax truyền thống.
  \item \textbf{Negative sampling}.
  \item \textbf{Subsampling frequent words}.
\end{itemize}

\subsection{Hierarchical softmax}
Trong cả CBOW model và Skip-gram model ban đầu, chúng ta đều sử dụng hàm softmax để tính toán xác suất để một từ là đầu ra chính xác. Nhưng việc tính toán xác suất bằng hàm softmax rất tốn kém về mặt tài nguyên tính toán. Khi chúng ta đào tạo một CBOW model, chúng ta cần tính toán xác suất của vocabulary thứ $j$ để nó là một target word chính xác:
$$\boldsymbol{y}_j = \dfrac{\exp{(u_j)}}{\sum^V_{j'=1} \exp{(u'_j)}}$$

\indent Nhìn vào phương trình trên, cơ bản là chúng ta đang tính toán hàm mũ cho tất cả các từ $\boldsymbol{u'}_j$ trong corpus của chúng ta. Độ phức tạp của thuật toán lúc này là $O(V)$. Khi trong corpus của chúng ta có đến hàng triệu từ nó sẽ chắc chắn xảy ra tốn kém về mặt tính toán. Vì vậy, để khắc phục vấn đề này, thay vì tính toán hàm softmax một cách truyền thống người ta thực hiện tính toán softmax trên cây.\\

\indent Hierarchical softmax sử dụng \textbf{Huffman binary search tree} \textit{(cây tìm kiếm nhị phân Huffman)} để giảm độ phức tạp về mặt tính toán xuống $O(\log_2(V))$. Lúc này kiến trúc mạng có sự thay đổi đáng kể ở output layer - chúng ta sẽ thay output layer bằng một \textbf{binary search tree}.

\includeImage{0.99}{11}{Thay thế output layer bằng Hierarchical softmax trong CBOW model.}

\indent Mỗi node lá bây giờ trong cây đại diện cho một từ trong corpus và tất cả các node trung gian đại diện cho \textbf{relative probability} \textit{xác suất tương đối} của tất cả các node con của chúng.\\

\indent Như vậy, làm sao để chúng ta tính toán xác suất của một target word khi được cung cấp context word. Chúng ta chỉ cần duyệt cây và đưa ra quyết định nên duyệt cây bên trái hay bên phải tại một node nhất định. Như thể hiện trong hình dưới đây, chúng ta tính xác suất của từ $\textsl{flew}$ là target word dựa vào $c$ context word. Chúng ta lấy tích của tất cả các xác suất dọc theo con đường đến target word.
\includeImage{0.8}{12}{Tính xác suất của target word trên Hierarchical softmax.}

$$p(\textsl{flew}|c) = p_{n_0}(left|c) \times p_{n_1}(right|c) = 0.6 \times 0.8 = 0.12$$

\subsection{Negative sampling}
Xem xét câu: \textsl{"Birds are flying in the sky."} với target word là \textsl{flying} và còn lại là các context word. Chúng ta cần cập nhật lại các weight trong network mỗi khi nó dự đoán một target word không chính xác. Vì vậy, ngoại trừ từ \textsl{flying}, nếu một từ khác được dự đoán là target word thì chúng ta sẽ cập nhật network.\\

\indent Hãy xem xét trường hợp chúng ta có hàng triệu vocabulary trong corpus. Trong trường hợp này, chúng ta cần phải thực hiện hàng triệu lần cập nhật weight cho đến khi đạt được optimal weight - lúc này nó tốn rất nhiều tài nguyên và chi phí.\\

\indent Để khắc phục điều này, chúng ta sẽ đánh dấu correct target word là positive class và lấy mẫu một vài từ còn lại thuộc negative class. Về cơ bản, chúng ta đang chuyển về bài toán phân loại nhị phân. Lúc này, xác suất để một từ được chọn là negative word.
$$p(w_i) = \dfrac{frequency(w_i)^{\frac{3}{4}}}{\sum^n_{j=0} frequency(w_j)^{\frac{3}{4}}}$$

\subsection{Subsampling frequent words}
Trong corpus, sẽ có một số từ xuất hiện rất thường xuyên chẳng hạn như \textsl{the}, \textsl{is},... và có một số từ lại xuất hiện không thường xuyên cho lắm. Để duy trì sự cân bằng giữa hai yếu tố, người ta sử dụng kỹ thuật \textbf{subsampling} \textit{(lấy mẫu con)}. Vậy nên, chúng ta sẽ loại bỏ một số từ xuất hiện thường xuyên hơn một \textbf{threshold} nhất định với xác suất $p$ được tính như sau:
$$p(w_i) = 1 - \sqrt{\dfrac{t}{f(w_i)}}$$

\twiceIndent ở đây $t$ là threshold và $f(w_i)$ là frequency của tử $i$.

\section{Mã nguồn}
\subsection{Xây dựng English Word2Vec model bằng Gensim}
Ở đây, em sử dụng Gensim package để xây dựng Word2Vec model với data là file \textsf{data/text.csv}. Gensim là một package được sử dụng rộng rãi để xây dựng các Word2Vec model. Trước tiên cần cài đặt Gensim package.
\begin{lstlisting}[language=bash]
# bash
pip install gensim
\end{lstlisting}

\vskip 0.5cm
\indent Import các thư viện cần thiết.
\begin{lstlisting}[language=python]
import warnings
warnings.filterwarnings('ignore')

# data processing
import pandas as pd
import re
from nltk.corpus import stopwords
stopWords = stopwords.words('english')

# modelling
from gensim.models import Word2Vec
from gensim.models import Phrases
from gensim.models.phrases import Phraser
\end{lstlisting}

\vskip 0.5cm
\indent Đọc dữ liệu được lưu trong file \textsf{data/text.csv}, có kích thước khoảng 23 MB.
\begin{lstlisting}[language=python]
data = pd.read_csv('data/text.csv', header=None)
\end{lstlisting}

\vskip 0.5cm
\indent Xem qua vài quan sát trong dữ liệu.
\begin{lstlisting}[language=python]
data.head()
\end{lstlisting}
\includeImage{1}{14}{}

\vskip 0.5cm
\indent Định nghĩa hàm dùng để tiền xử lí dữ liệu.
\begin{lstlisting}[language=python]
def pre_process(text):    
  '''
  Lowercase text '''
  text = str(text).lower()
  
  ''' 
  Remove các kí tự đặc biệt, số, khoảng trắng thừa '''
  text = re.sub(r'[^A-Za-z0-9\s.]', r'', text)
  
  '''
  Remove new line characters '''
  text = re.sub(r'\n',r' ',text)
  
  '''
  Remove stopwords '''
  text = " ".join([word for word in text.split() if word not in stopWords])
  
  return text
\end{lstlisting}

\vskip 0.5cm
\indent Thử áp dụng hàm \texttt{pre\_process()} lên một mẫu dữ liệu.
\begin{lstlisting}[language=python]
pre_process(data[0][50])
\end{lstlisting}
\includeImage{1}{15}{}

\vskip 0.5cm
\indent Pre-processing trên toàn bộ data.\
\begin{lstlisting}[language=python]
data[0] = data[0].map(lambda x: pre_process(x))
\end{lstlisting}

\vskip 0.5cm
\indent Xem lại dữ liệu sau khi pre-processing.
\begin{lstlisting}[language=python]
data[0].head()
\end{lstlisting}
\includeImage{1}{16}{}

\vskip 0.5cm
\indent Gensim yêu cầu input có dạng list of lists. Mỗi quan sát trong dữ liệu là một set of sentences. Cho nên ta cần tách các sentence bằng dấu \texttt{'.'} sau đó bỏ chúng vào list. Ví dụ: text = [[word1, word2, word3], [word1, word2, word3]]
\begin{lstlisting}[language=python]
data[0][1].split('.')[:5]
\end{lstlisting}
\includeImage{1}{17}{}

\vskip 0.5cm
\indent Chúng ta đã có một data là một list of sentence. Bây giờ chúng ta cần tách các sentence thành list of words dựa vào kí tự \texttt{' '}.
\begin{lstlisting}[language=python]
corpus = []
for line in data[0][1].split('.'):
  words = [x for x in line.split()]
  corpus.append(words)
\end{lstlisting}

\vskip 0.5cm
\indent Bây giờ, input của chúng ta đã có dạng chuẩn của Gensim yêu cầu là list of lists.
\begin{lstlisting}[language=python]
corpus[:2]
\end{lstlisting}
\includeImage{1}{18}{}

\vskip 0.5cm
\indent Các đoạn code trên chúng ta chỉ thử trên một sentence, bây giờ chúng ta sẽ chuyển đổi toàn bộ dataset.
\begin{lstlisting}[language=python]
data = data[0].map(lambda x: x.split('.'))

corpus = []
for i in (range(len(data))):
  for line in data[i]:
    words = [x for x in line.split()]
    corpus.append(words)

corpus[:2]
\end{lstlisting}
\includeImage{1}{19}{}

\vskip 0.5cm
\indent Giả sử chúng ta có từ \texttt{'new\ york'}, chúng ta sẽ thêm dấu gạch \texttt{'\_'} để thay cho khoảng trắng. Chúng ta thiết lập \texttt{min\_count=25}, tức ta bỏ qua tất cả các từ có tần số xuất hiện dưới 25.
\begin{lstlisting}[language=python]
phrases = Phrases(sentences=corpus, min_count=25, threshold=50)
bigram = Phraser(phrases)

for index,sentence in enumerate(corpus):
  corpus[index] = bigram[sentence]
\end{lstlisting}

\vskip 0.5cm
\indent Kiểm tra dữ liệu.
\begin{lstlisting}[language=python]
corpus[111]
\end{lstlisting}
\includeImage{1}{20}{}

\vskip 0.5cm
\indent Bây giờ chúng ta sẽ build model word2vec, trước tiên chúng ta cần xác định một vài hyperparams.
\begin{itemize}
  \item \texttt{vector\_size}: kích thước của embedding vector. Tùy chỉnh dựa vào kích thước của dataset mà ta có.
  \item \texttt{window\_size}: kích thước của cửa sổ context words.
  \item \texttt{min\_count}: tần số thấp nhất của từ trong dataset.
  \item \texttt{workers}: chỉ định số nhân CPU dùng để training model. 
  \item \texttt{sg}: bằng $1$ sẽ dùng skip-gram method để training, nếu $0$ thì chỉ định CBOW method để training.
\end{itemize}
\begin{lstlisting}[language=python]
vector_size = 100
window_size = 2
epochs = 100
min_count = 2
workers = 4
sg = 1
\end{lstlisting}

\vskip 0.5cm
\indent Training model.
\begin{lstlisting}[language=python]
  model = Word2Vec(corpus, sg=sg, window=window_size, vector_size=vector_size, min_count=min_count, workers=workers, epochs=epochs)
\end{lstlisting}

\vskip 0.5cm
\indent Lưu lại Word2Vec model vào folder \texttt{model}.
\begin{lstlisting}[language=python]
model.save('./model/word2vec.model')
\end{lstlisting}

\vskip 0.5cm
\indent Đánh giá Word2Vec model. Gensim cung cấp phương thức \texttt{most\_similar()} sẽ cho ta biết những từ nào tương đồng với một từ được cung cấp. Dưới đây từ \texttt{san\_deigo} được cung cấp là đầu vào, ta cần tìm các từ mà có sự tương đồng với từ này.
\begin{lstlisting}[language=python]
model.wv.most_similar('san_diego')
\end{lstlisting}
\includeImage{1}{21}{}

\vskip 0.5cm
\indent Có thể áp dụng các phép toán số học trên vector, ví dụ:
$$woman + king - man = queen$$
\begin{lstlisting}[language=python]
model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
\end{lstlisting}
\includeImage{1}{22}{}

\vskip 0.5cm
\indent Chúng ta có thể tìm hiểu xem từ nào là không phù hợp với đa số các từ được cung cấp trong list.
\begin{lstlisting}[language=python]
text = ['los_angeles','indianapolis', 'holiday', 'san_antonio','new_york']
model.wv.doesnt_match(text)
\end{lstlisting}
\includeImage{1}{23}{}

\vskip 0.5cm
\indent Sử dụng tensorboard để trực quan hóa word2vec. Import các thư viện cần thiết. 
\begin{lstlisting}[language=python]
import warnings
warnings.filterwarnings(action='ignore')


import tensorflow as tf
from tensorflow.contrib.tensorboard.plugins import projector
tf.logging.set_verbosity(tf.logging.ERROR)

import numpy as np
import gensim
import os
\end{lstlisting}

\vskip 0.5cm
\indent Load model Word2Vec.
\begin{lstlisting}[language=python]
file_name = "./model/word2vec.model"
model = gensim.models.keyedvectors.KeyedVectors.load(file_name)
\end{lstlisting}

\vskip 0.5cm
\indent Sau khi load model thành công, chúng ta cần lưu lại số từ vựng có trong model.
\begin{lstlisting}[language=python]
max_size = len(model.wv.key_to_index.keys())-1
\end{lstlisting}

\vskip 0.5cm
\indent Chúng ta biết rằng số chiều của các word vector là $\boldsymbol{V} \times \boldsymbol{N}$. Tức số từng vựng $\boldsymbol{V} \times$ số neuron trong hidden layer $\boldsymbol{N}$. Bây giờ chúng ta sẽ tạo ma trận $\boldsymbol{W}$ và lưu vào biến \texttt{w2v}.
\begin{lstlisting}[language=python]
w2v = np.zeros((max_size, model.layer1_size))
\end{lstlisting}

\vskip 0.5cm
\indent Tiếp theo, chúng ta sẽ lưu các từ cùng các word vectors vào file \texttt{metadata.tsv}.
\begin{lstlisting}[language=python]
if not os.path.exists('projections'):
  os.makedirs('projections')
  
with open("projections/metadata.tsv", 'w+') as file_metadata:    
  for i, word in enumerate(model.wv.index_to_key[:max_size]):
    # lưu embedding vector
    w2v[i] = model.wv[word]
      
    # lưu word tương ứng 
    file_metadata.write(word + '\n')
\end{lstlisting}

\vskip 0.5cm
\indent Khởi tạo TensorFlow session.
\begin{lstlisting}[language=python]
sess = tf.InteractiveSession()
\end{lstlisting}

\vskip 0.5cm
\indent Tạo biến \texttt{embeddings} để chứa các từ nhúng và các thiết lập cần thiết cho TensorBoard.
\begin{lstlisting}[language=python]
with tf.device("/gpu:0"):
  embedding = tf.Variable(w2v, trainable=False, name='embedding')

# cấu hình cần thiết
tf.global_variables_initializer().run()
saver = tf.train.Saver()
writer = tf.summary.FileWriter('projections', sess.graph)

config = projector.ProjectorConfig()
embed = config.embeddings.add()
\end{lstlisting}

\vskip 0.5cm
\indent Chỉ định tập tin \texttt{metadata.tsv} để trực quan hóa.
\begin{lstlisting}[language=python]
embed.tensor_name = 'embedding'
embed.metadata_path = 'metadata.tsv'
\end{lstlisting}

\vskip 0.5cm
\indent Lưu lại model dùng để visualization của TensorBoard.
\begin{lstlisting}[language=python]
projector.visualize_embeddings(writer, config)
saver.save(sess, 'projections/model.ckpt', global_step=max_size)
\end{lstlisting}

\vskip 0.5cm
\indent Mở terminal và nhập lệnh dưới đây để xem kết quả trực quan.
\begin{lstlisting}[language=bash]
# bash
tensorboard --logdir=projections --port=8000
\end{lstlisting}
\includeImage{1}{24}{}

\begin{tcolorbox}[grow to left by=-0.6cm]
  \textbf{Nhận xét}
  \begin{itemize}
    \item Các từ này được giảm chiều từ 100 xuống 3 bằng PCA và biểu diễn trong không gian 3 chiều.
    \item Khi ta tìm từ \texttt{'san\_diego'} ta sẽ thấy được các từ được cho là tương tự với từ này trong TensorBoard (các từ nằm trong vùng xanh).
  \end{itemize}
\end{tcolorbox}



\subsection{Xây dựng Word2Vec model sử dụng build-in model của Báo Mới}
Import các package cần thiết.
\begin{lstlisting}[language=python]
import warnings
warnings.filterwarnings('ignore')

# data processing
import pandas as pd
import re
from nltk.corpus import stopwords
stopWords = stopwords.words('english')

# modelling
from gensim.models import Word2Vec, KeyedVectors
from gensim.models import Phrases
from gensim.models.phrases import Phraser
\end{lstlisting}

\vskip 0.5cm
\indent Load Vietnamese Word2Vec model.
\begin{lstlisting}[language=python]
  model = KeyedVectors.load_word2vec_format('./model/baomoi.vn.model.bin', binary=True)
\end{lstlisting}

\vskip 0.5cm
\indent Đánh giá Word2Vec model. Gensim cung cấp phương thức \texttt{most\_similar()} sẽ cho ta biết những từ nào tương đồng với một từ được cung cấp. Dưới đây từ \texttt{san\_deigo} được cung cấp là đầu vào, ta cần tìm các từ mà có sự tương đồng với từ này.
\begin{lstlisting}[language=python]
model.most_similar('khoa_học_tự_nhiên')
\end{lstlisting}
\includeImage{1}{25}{}

\vskip 0.5cm
\indent Có thể áp dụng các phép toán số học trên vector, ví dụ:
$$\text{toán} + \text{khoa\_học} - \text{kinh\_tế} = \text{toán\_tin}$$

\begin{lstlisting}[language=python]
  model.most_similar(positive=['toán', 'khoa_học'], negative=['kinh_tế'], topn=10)
\end{lstlisting}
\includeImage{1}{26}{}

\vskip 0.5cm
\indent Chúng ta có thể tìm hiểu xem từ nào là không phù hợp với đa số các từ được cung cấp trong list.
\begin{lstlisting}[language=python]
  text = ['máy_tính','khoa_học_máy_tính', 'công_nghệ_phần_mềm', 'kinh_tế', 'đại_số']
model.doesnt_match(text)
\end{lstlisting}
\includeImage{1}{27}{}

\vskip 0.5cm
\indent Data visualization cho Word2Vec model của Báo Mới. Import các package cần thiết.
\begin{lstlisting}[language=python]
import warnings
warnings.filterwarnings(action='ignore')


import tensorflow as tf
from tensorflow.contrib.tensorboard.plugins import projector
tf.logging.set_verbosity(tf.logging.ERROR)

import numpy as np
import gensim
import os
\end{lstlisting}

\vskip 0.5cm
\indent Load Word2Vec model
\begin{lstlisting}[language=python]
  file_name = "./model/baomoi.vn.model.bin"
  model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(file_name, binary=True)
\end{lstlisting}

\vskip 0.5cm
\indent Sau khi load model thành công, chúng ta cần lưu lại số từ vựng có trong model.
\begin{lstlisting}[language=python]
  max_size = len(model.key_to_index.keys())-1
\end{lstlisting}

\vskip 0.5cm
\indent Chúng ta biết rằng số chiều của các word vector là $\boldsymbol{V} \times \boldsymbol{N}$. Tức số từng vựng $\boldsymbol{V} \times$ số neuron trong hidden layer $\boldsymbol{N}$. Bây giờ chúng ta sẽ tạo ma trận $\boldsymbol{W}$ và lưu vào biến \texttt{w2v}.
\begin{lstlisting}[language=python]
  w2v = np.zeros((max_size, model.vector_size))
\end{lstlisting}

\vskip 0.5cm
\indent Tiếp theo, chúng ta sẽ lưu các từ cùng các word vectors vào file \texttt{metadata.tsv}.
\begin{lstlisting}[language=python]
  if not os.path.exists('projections_vi'):
  os.makedirs('projections_vi')
  
  with open("projections_vi/metadata.tsv", 'w+') as file_metadata:    
  for i, word in enumerate(model.index_to_key[:max_size]):
  # lưu embedding vector
  w2v[i] = model[word]
  
  # lưu word tương ứng 
  file_metadata.write(word + '\n')
\end{lstlisting}


\vskip 0.5cm
\indent Khởi tạo TensorFlow session.
\begin{lstlisting}[language=python]
sess = tf.InteractiveSession()
\end{lstlisting}

\vskip 0.5cm
\indent Tạo biến \texttt{embeddings} để chứa các từ nhúng và các thiết lập cần thiết cho TensorBoard.
\begin{lstlisting}[language=python]
  with tf.device("/gpu:0"):
  embedding = tf.Variable(w2v, trainable=False, name='embedding')
  
  # run TensorFlow session and set up some configuration
  tf.global_variables_initializer().run()
  saver = tf.train.Saver()
  writer = tf.summary.FileWriter('projections_vi', sess.graph)
  
  config = projector.ProjectorConfig()
  embed = config.embeddings.add()
\end{lstlisting}

\vskip 0.5cm
\indent Chỉ định tập tin để \texttt{metadata.tsv} để trực quan hóa bằng TensorBoard.
\begin{lstlisting}[language=python]
  embed.tensor_name = 'embedding'
  embed.metadata_path = 'metadata.tsv'
\end{lstlisting}

\vskip 0.5cm
\indent Lưu lại model dùng để visualization của TensorBoard.
\begin{lstlisting}[language=python]
projector.visualize_embeddings(writer, config)
saver.save(sess, 'projections_vi/model.ckpt', global_step=max_size)
\end{lstlisting}

\vskip 0.5cm
\indent Mở terminal và nhập lệnh dưới đây để trực quan hóa.
\begin{lstlisting}[language=bash]
  # bash
  tensorboard --logdir=projections_vi --port=8000
\end{lstlisting}
\includeImage{1}{28}{}

\begin{tcolorbox}[grow to left by=-0.6cm]
  \textbf{Nhận xét}
  \begin{itemize}
    \item Các từ này được giảm chiều từ 300 xuống 3 bằng PCA và biểu diễn trong không gian 3 chiều.
    \item Khi ta tìm từ \texttt{'khoa\_học\_tự\_nhiên'} ta sẽ thấy được các từ được cho là tương tự với từ này trong TensorBoard (các từ nằm trong vùng xanh).
  \end{itemize}
\end{tcolorbox}

\section{Tài nguyên tham khảo}
\begin{itemize}
  \item \textbf{\color{teal} Hands-On Deep Learning Algorithms with Python} - \textit{(chương 7)}, link sách tại \href{https://www.amazon.com/Hands-Deep-Learning-Algorithms-Python/dp/1789344158}{\color{blue} https://www.amazon.com/Hands-Deep-Learning-Algorithms-Python/dp/1789344158}.
  \item \textbf{\color{teal} Natural Language Processing in Action}, link sách tại \href{https://www.amazon.com/Natural-Language-Processing-Action-Understanding/dp/1617294632}{\color{blue} https://www.amazon.com/Natural-Language-Processing-Action-Understanding/dp/1617294632}. 
\end{itemize}

\end{document}
