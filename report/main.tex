\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage[utf8]{vietnam}
\usepackage{tipa}
\usepackage{float}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  literate=%
    {đ}{{\dj}}1
    {â}{{\^a}}1
    {ă}{{\u{a}}}1
    {ê}{{\^e}}1
    {ô}{{\^o}}1
    {ơ}{{\ohorn}}1
    {ư}{{\uhorn}}1
    {á}{{\'a}}1
    {à}{{\`a}}1
    {ả}{\h{a}}1
    {ã}{{\~a}}1
    {ạ}{\textsubdot{a}}1
    {ấ}{\'{\^a}}1
    {ầ}{\`{\^a}}1
    {ẩ}{\h{\^a}}1
    {ẫ}{\~{\^a}}1
    {ậ}{\textsubdot{\^a}}1
    {ắ}{\'{\u{a}}}1
    {ằ}{\`{\u{a}}}1
    {ẳ}{\h{\u{a}}}1
    {ẵ}{\~{\u{a}}}1
    {ặ}{\textsubdot{\u{a}}}1
    {é}{{\'e}}1
    {è}{{\`e}}1
    {ẻ}{\h{e}}1
    {ẽ}{{\~e}}1
    {ẹ}{\textsubdot{e}}1
    {ế}{\'{\^e}}1
    {ề}{\`{\^e}}1
    {ể}{\h{\^e}}1
    {ễ}{\~{\^e}}1
    {ệ}{\textsubdot{\^{e}}}1
    {í}{{\'i}}1
    {ì}{{\`i}}1
    {ỉ}{\h{i}}1
    {ĩ}{{\~i}}1
    {ị}{\textsubdot{i}}1
    {ó}{{\'o}}1
    {ò}{{\`o}}1
    {ỏ}{\h{o}}1
    {õ}{{\~o}}1
    {ọ}{\textsubdot{o}}1
    {ố}{\'{\^o}}1
    {ồ}{\`{\^o}}1
    {ổ}{\h{\^o}}1
    {ỗ}{\~{\^o}}1
    {ộ}{\textsubdot{\^o}}1
    {ớ}{\'{\ohorn}}1
    {ờ}{\`{\ohorn}}1
    {ở}{\h{\ohorn}}1
    {ỡ}{\~{\ohorn}}1
    {ợ}{\textsubdot{\ohorn}}1
    {ú}{{\'u}}1
    {ù}{{\`u}}1
    {ủ}{\h{u}}1
    {ũ}{{\~u}}1
    {ụ}{\textsubdot{u}}1
    {ứ}{\'{\uhorn}}1
    {ừ}{\`{\uhorn}}1
    {ử}{\h{\uhorn}}1
    {ữ}{\~{\uhorn}}1
    {ự}{\textsubdot{\uhorn}}1
    {ý}{{\'y}}1
    {ỳ}{{\`y}}1
    {ỷ}{\h{y}}1
    {ỹ}{{\~y}}1
    {ỵ}{\textsubdot{y}}1
    {Đ}{{\DJ}}1
    {Â}{{\^A}}1
    {Ă}{{\u{A}}}1
    {Ê}{{\^E}}1
    {Ô}{{\^O}}1
    {Ơ}{{\OHORN}}1
    {Ư}{{\UHORN}}1
    {Á}{{\'A}}1
    {À}{{\`A}}1
    {Ả}{\h{A}}1
    {Ã}{{\~A}}1
    {Ạ}{\textsubdot{A}}1
    {Ấ}{\'{\^A}}1
    {Ầ}{\`{\^A}}1
    {Ẩ}{\h{\^A}}1
    {Ẫ}{\~{\^A}}1
    {Ậ}{\textsubdot{\^A}}1
    {Ắ}{\'{\u{A}}}1
    {Ằ}{\`{\u{A}}}1
    {Ẳ}{\h{\u{A}}}1
    {Ẵ}{\~{\u{A}}}1
    {Ặ}{\textsubdot{\u{A}}}1
    {É}{{\'E}}1
    {È}{{\`E}}1
    {Ẻ}{\h{E}}1
    {Ẽ}{{\~E}}1
    {Ẹ}{\textsubdot{E}}1
    {Ế}{\'{\^E}}1
    {Ề}{\`{\^E}}1
    {Ể}{\h{\^E}}1
    {Ễ}{\~{\^E}}1
    {Ệ}{\textsubdot{\^{E}}}1
    {Í}{{\'I}}1
    {Ì}{{\`I}}1
    {Ỉ}{\h{I}}1
    {Ĩ}{{\~I}}1
    {Ị}{\textsubdot{I}}1
    {Ó}{{\'O}}1
    {Ò}{{\`O}}1
    {Ỏ}{\h{O}}1
    {Õ}{{\~O}}1
    {Ọ}{\textsubdot{O}}1
    {Ố}{\'{\^O}}1
    {Ồ}{\`{\^O}}1
    {Ổ}{\h{\^O}}1
    {Ỗ}{\~{\^O}}1
    {Ộ}{\textsubdot{\^O}}1
    {Ớ}{\'{\OHORN}}1
    {Ờ}{\`{\OHORN}}1
    {Ở}{\h{\OHORN}}1
    {Ỡ}{\~{\OHORN}}1
    {Ợ}{\textsubdot{\OHORN}}1
    {Ú}{{\'U}}1
    {Ù}{{\`U}}1
    {Ủ}{\h{U}}1
    {Ũ}{{\~U}}1
    {Ụ}{\textsubdot{U}}1
    {Ứ}{\'{\UHORN}}1
    {Ừ}{\`{\UHORN}}1
    {Ử}{\h{\UHORN}}1
    {Ữ}{\~{\UHORN}}1
    {Ự}{\textsubdot{\UHORN}}1
    {Ý}{{\'Y}}1
    {Ỳ}{{\`Y}}1
    {Ỷ}{\h{Y}}1
    {Ỹ}{{\~Y}}1
    {Ỵ}{\textsubdot{Y}}1}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{indentfirst}
\lstset{basicstyle=\ttfamily, showstringspaces=false, commentstyle=\color{red}, keywordstyle=\color{blue}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
            {-2.5ex\@plus -1ex \@minus -.25ex}
            {1.25ex \@plus .25ex}
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand{\twiceIndent}{\hspace{\parindent}}
\newcommand{\leftIndent}{\setlength{\parindent}{0.6cm}}
\newcommand{\lleftIndent}{\setlength{\parindent}{1.2cm}}
\newcommand{\itemsizePaddingLeft}{\setlength{\itemindent}{0.6cm}}
\newcommand{\includeImage}[3]{
\begin{figure}[H]
  \centering
  \includegraphics[width=#1\textwidth]{images/#2.png}
  \def\temp{#3}\ifx\temp\empty\else\caption{#3}\fi
\end{figure}}

\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
  
  \center
  
  \textsc{\LARGE Đại học Khoa học tự nhiên}\\[1.5cm] 
  \textsc{\Large Môn học: Khoa Học Dữ Liệu}\\[0.5cm] 
  \textsc{\large Báo cáo đồ án cuối kỳ}\\[0.5cm] 
  
  \HRule \\[0.4cm]
  { \huge \bfseries Word2Vec}\\[0.4cm]
  \HRule \\[1.5cm]

  \begin{minipage}{0.4\textwidth}
  \begin{flushleft} \large
  \emph{\textbf{Giảng viên}}\\
  Nguyễn Ngọc Đức 
  \end{flushleft}
  \end{minipage}
  ~
  \begin{minipage}{0.4\textwidth}
  \begin{flushright}
    \large \emph{\\[3cm]\textbf{Sinh viên}} \\
    20424008 - Dương Mạnh Cường
  \end{flushright}
  \end{minipage}\\[2cm]

  \includegraphics{./images/logo_hcmus.png}\\[1cm]
  \vfill

  \today
  
  \end{titlepage}

\tableofcontents
\newpage

Neural network yêu cầu đầu vào ở dạng \textbf{numeric}. Cho nên, khi ta có dữ liệu dạng \textbf{text}, ta cần chuyển đổi chúng thành dữ liệu dạng numeric.\\

\indent Có nhiều phương pháp khác nhau để chuyển đổi dữ liệu dạng text sang numeric mà phổ biến nhất là:
\begin{itemize}
  \setlength{\itemindent}{0.6cm}
  \item Term frequency-inverse document frequency (TF-IDF).
  \item Bag of words (BOW).
\end{itemize}

\vskip 0.5cm
\indent Tuy nhiên, điểm yếu của hai phương pháp trên là chúng \textbf{không nắm bắt được ngữ nghĩa của từ}, nói cách khác là chúng không hiểu được ý nghĩa của từ.\\

\indent Có nhiều cách khắc phục nhược điểm này, mà một trong những cách đó là sử dụng \textbf{Word2Vec} bằng cách đại diện cho từng từ bằng một \textbf{vector} trong không gian $m$ chiều. Lúc này, các từ có nghĩa tương đồng nhau sẽ nằm gần nhau.

\includeImage{0.4}{01}{Các từ có ý nghĩa tương đồng nhau nằm gần nhau.}

\section{Word2Vec model}
Word2Vec là một trong những phương pháp \textbf{word embedding} được sử dụng phổ biến.\\

\indent Word embedding là cách ta biểu diễn các \textbf{word vector} trong không gian vector.\\

\indent Các word vector được tạo ra bởi Word2Vec model có khả năng nắm bắt được các \textbf{semantic} \textit{(ngữ nghĩa)} và \textbf{syntactic} \textit{(ý nghĩa cú pháp)} của từ.\\

\includeImage{1}{03}{Ví dụ về \textbf{semantic} và \textbf{syntactic}.}

\indent Ví dụ có câu: \textsl{"Archie used to live in New York, he then moved to Santa Clara. He loves apples and strawberries."}.\\

\indent Word2Vec model sẽ phát sinh các vector cho từng từ trong văn bản. Nếu chúng ta trực quan các vector này trong không gian vector tương ứng, chúng ta có thể thấy các từ tương tự nhau sẽ nằm gần nhau.

\includeImage{0.95}{02}{Các từ trong câu ví dụ được biểu diễn trong không gian vector}
\begin{tcolorbox}[grow to left by=-0.6cm]
  \textbf{Nhận xét}
  \begin{itemize}
    \item Ở đây các cặp từ như \textsl{apples} - \textsl{strawberries}, \textsl{New York} - \textsl{Santa Clara} có ý nghĩa tương đồng nhau nên nằm gần nhau.
  \end{itemize}
\end{tcolorbox}

\vskip 0.5cm
\indent Do đó, với Word2Vec model có thể học cách biểu diễn các vector giúp cho neural network có thể hiểu được ý nghĩa của từ tương ứng với vector đó.\\

\indent Và vì chúng ta có thể hiểu được semantic và syntactic của từ điều này giúp ta tận dụng các vector này vào các bài toán như \textbf{text summarization} \textit{(tóm tắt văn bản)}, \textbf{sentiment analysis} \textit{(phân tích tình cảm)}, \textbf{text generation} \textit{(tạo văn bản)},...\\

\indent Có hai cách để xây dựng Word2Vec model:
\begin{enumerate}
  \itemsizePaddingLeft
  \item \textbf{CBOW model}.
  \item \textbf{Skip-gram model}.
\end{enumerate}

\subsection{CBOW model}
Giả sử chúng ta có một neural network bao gồm: \textbf{một input layer}, \textbf{một hidden layer} và \textbf{một output layer}. Mục đích của network này là dự đoán ra \textbf{một từ} dựa vào \textbf{các từ xung quanh nó}. Từ mà chúng ta cố gắng dự đoán được gọi là \textbf{target word} và các từ xung quanh nó được gọi là \textbf{context word}.\\

\indent Vậy chúng ta cần bao nhiêu context word để dự đoán ra target word? Chúng ta sẽ sử dụng một \textbf{window} \textit{(cửa sổ)} có kích thước là $n$ để chọn các context word. Nếu $n = 2$ thì chúng ta sẽ sử dụng hai từ \textbf{phía trước} và \textbf{phía sau} của target word làm các context word.\\

\indent Xem xét câu sau: \textsl{"The Sun rises in the east."} với \textsl{rises} là target word. Nếu chúng ta xét kích thước của window là $2$ thì chúng ta sẽ lấy hai từ phía trước là \textsl{The}, \textsl{Sun} và hai từ phía sau là \textsl{in}, \textsl{the} của target word làm các context word.\\

\includeImage{0.7}{04}{Target word và context word trong CBOW model.}

\indent Lúc này input của network là các context word và output của network là target word. Và vì neural network chỉ chấp nhận input là numeric data nên chúng ta sẽ sử dụng kỹ thuật \textbf{one-hot encoding} để chuyển đổi các text data thành numeric data.

\includeImage{0.6}{05}{One-hot encoding cho text data.}

\indent Kiến trúc của CBOW model được thể hiện dưới hình sau. Ở đây các context word là: \textsl{the}, \textsl{sun}, \textsl{in} và \textsl{east} được dùng làm đầu vào cho network và dự đoán ra target word là \textsl{rises} ở đầu ra.

\includeImage{0.99}{06}{Kiến trúc của CBOW model 1.}

\indent Ở vài lần lặp đầu tiên, network không thể dự đoán target word một cách chính xác. Nhưng sau một loạt các vòng lặp bằng cách sử dụng \textbf{gradient descent}, các \textbf{weight} \textit{(trọng số)} của network được cập nhật và tìm ra được \textbf{optimal weight} \textit{(trọng số thích hợp)} để dự đoán ra target word một cách chính xác.

\vskip 0.5cm
\indent Vì chúng ta có một input layer, một hidden layer và một output layer, nên chúng ta sẽ có hai weight:
\begin{enumerate}
  \itemsizePaddingLeft
  \item Weight từ input layer đến hidden layer - $\boldsymbol{W}$.
  \item Weight từ hidden layer đến output layer - $\boldsymbol{W'}$.
\end{enumerate}

\indent Trong quá trình đào tạo network, các weight sẽ được cập nhật trong quá trình back propagation nhằm tìm ra optimal weight cho hai bộ $\boldsymbol{W}$ và $\boldsymbol{W'}$.

\vskip 0.5cm
\indent Về sau, weight set giữa input layer và hidden layer $\boldsymbol{W}$ được cập nhật và tối ưu tạo thành các vector đại diện cho các từ của input layer.\\

\indent Sau khi kết thúc quá trình đào tạo, chúng ta chỉ cần loại bỏ output layer và lấy ra weight set giữa input layer và hidden layer và gán chúng cho các từ tương ứng.\\

\indent Dưới đây là các vector tương ứng cho các từ của $\boldsymbol{W}$. Word embedding tương ứng cho từ \textsl{sun} là $\begin{bmatrix} 0.0 & 0.3 & 0.3 & 0.6 & 0.1 \\ \end{bmatrix}$.

$$\boldsymbol{W} = \text{ } \begin{matrix} the \\ sun \\ rises \\ in \\ east \end{matrix} \begin{bmatrix} 0.01 & 0.02 & 0.1 & 0.5 & 0.37 \\ 0.0 & 0.3 & 0.3 & 0.6 & 0.1 \\ 0.4 & 0.34 & 0.11 & 0.61 & 0.43 \\ 0.1 & 0.11 & 0.1 & 0.17 & 0.369 \\ 0.33 & 0.4 & 0.3 & 0.17 & 0.1 \\ \end{bmatrix}$$

\subsubsection{CBOW model với một context word}
CBOW model cần một số lượng context word $C$ nhất định để dự đoán target word. Ở phần này chúng ta sẽ xem xét trường hợp chỉ sử dụng duy nhất một context word, tức $C = 1$. Lúc này netword nhận vào một context word ở đầu vào và trả về một target word ở đầu ra.\\

\indent Trước tiên, chúng ta cần làm quen với một vài khái niệm. Tất cả các từ duy nhất nằm trong \textbf{corpus} \textit{(kho ngữ liệu)} của chúng ta được gọi là các \textbf{vocabulary} \textit{(từ vựng)}. Trong ví dụ trước đó của chúng ta, chúng ta có năm từ là: \textsl{the}, \textsl{sun}, \textsl{rises}, \textsl{in} và \textsl{east} - toàn bộ năm từ này chính là vocabulary trong corpus của chúng ta.\\

\indent Đặt $V$ là số lượng vocabulary và $N$ là số neuron của hidden layer. Vì neural network của chúng ta có ba layer, cụ thể:
\begin{itemize}
  \itemsizePaddingLeft
  \item Input layer được đại diện bởi $\boldsymbol{X} = \{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_3,...,\boldsymbol{x}_k,...,\boldsymbol{x}_V \}$. Khi chúng ta nói $\boldsymbol{x}_k$, tức ta đang đề cập đến từ thứ $k$ trong corpus của chúng ta.
  \item Hidden layer được đại diện bởi $\boldsymbol{H} = \{\boldsymbol{h}_1,\boldsymbol{h}_2,\boldsymbol{h}_3,...,\boldsymbol{h}_i,...,\boldsymbol{h}_N \}$. Khi chúng ta nói $\boldsymbol{h}_i$, tức ta đang đề cập đến neuron thứ $i$ trong hidden layer.
  \item Output layer được đại diện bởi $\boldsymbol{Y} = \{\boldsymbol{y}_1,\boldsymbol{y}_2,\boldsymbol{y}_3,...,\boldsymbol{y}_j,...,\boldsymbol{y}_V \}$. Khi chúng ta nói $\boldsymbol{y}_j$, tức ta đang đề cập đến từ thứ $j$ trong output layer.
\end{itemize}

\vskip 0.5cm
\indent Số chiều của $\boldsymbol{W}$ là $V \times N$, tức $\textit{số vocabulary} \times \textit{số neuron trong hidden layer}$. $W_{ki}$ đại diện cho một phần tử trong ma trận $\boldsymbol{W}$ giữa $\boldsymbol{x}_k$ trong input layer và $\boldsymbol{h}_i$ trong hidden layer.

\vskip 0.5cm
\indent Số chiều của $\boldsymbol{W'}$ là $N \times V$, tức $\textit{số neuron trong hidden layer} \times \textit{số vocabulary}$. $W'_{ij}$ đại diện cho một phần tử trong ma trận $\boldsymbol{W'}$ giữa $\boldsymbol{h}_i$ trong hidden layer và $\boldsymbol{y}_j$ trong output layer.
     
\includeImage{0.7}{07}{Kiến trúc của CBOW model 2.}

\paragraph{Forward propagation}
Để dự đoán target word từ các context word, chúng ta sẽ thực hiện quá trình forward propagation. Trước tiên, chúng ta nhân $\boldsymbol{X}$ cho $\boldsymbol{W}$.
$$\boldsymbol{H} = \boldsymbol{XW}^T$$

\indent Chúng ta biết rằng input là các one-hot vector, nên khi ta nhân $\boldsymbol{X}$ với $\boldsymbol{W}$ thì $\boldsymbol{H}$ lúc này đơn giản là:
$$\boldsymbol{H} = \boldsymbol{W}_{(k,.)}$$

\indent $\boldsymbol{W}_{(k,.)}$ về cơ bản là biểu diễn vector của input word. Đặt vector đại diện cho input word $w_I$ bằng $Z_{w_I}$. Lúc này phương trình trên có thể được viết thành.\\
\begin{equation}
  \boldsymbol{H} = Z_{w_I}
\end{equation}

\indent Bây giờ chúng ta đang ở hidden layer, tiếp theo chúng ta cần tính xác suất cho từng từ trong corpus để nó là một target word.\\

\indent Đặt $\boldsymbol{u}_j$ là điểm số để từ thứ $j$ trong corpus là target word - được tính bằng cách nhân $\boldsymbol{H}$ cho $\boldsymbol{W'}$. Và vì chúng ta đang tính toán điểm số cho từ $j$ trong corpus, nên ta chỉ nhân cột $j$ trong $\boldsymbol{W'}$ cho $\boldsymbol{H}$.
$$\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$$
\indent Lúc này, cột $j$ của $\boldsymbol{W'}_{ij}$ đại diện cho vector của từ $j$ trong corpus. Đặt vector đại diện cho từ thứ $j$ là $Z'_{w_j^T}$. Phương trình trên có thể được viết lại thành:
\begin{equation}
  \boldsymbol{u}_j = Z'_{w_j^T} \cdot \boldsymbol{H}
\end{equation}

\indent Thế phương trình $(1)$ vào phương trình $(2)$, ta được:
$$\boldsymbol{u}_j = Z'_{w_j^T} \cdot Z_{w_I}$$

\indent Việc tính toán dot product giữa $Z'_{w_j^T}$ và $Z_{w_I}$ nói cho chúng ta biết độ tương đồng giữa target word với các context word đầu vào. Cho nên nếu điểm số của từ $j$ trong corpus cao thì có nghĩa có sự tương đồng cao giữa các context word và target word $j$ này và ngược lại.\\

\indent Cuối cùng, ta sẽ áp dụng softmax activation để chuyển đổi các điểm số này sang xác suất:
\begin{equation}
  \boldsymbol{y}_j = \dfrac{\exp{(\boldsymbol{u}}_j)}{\sum_{j'=1}^V \exp{(\boldsymbol{u}'_j)}}
\end{equation}

\indent Lúc này, $\boldsymbol{y}_j$ cho chúng ta biết xác suất của từ $j$ là target word. Chúng ta sẽ tính xác suất cho tất cả các từ trong corpus và chọn từ mà có xác suất cao nhất là target word.\\

\indent Tiếp theo, chúng ta sẽ tính toán hàm loss. Đặt $\boldsymbol{y}_j^*$ là xác suất để từ $j$ chính xác là target word. Chúng ta cần tối đa xác suất này:
$$\max{\boldsymbol{y}_j^*}$$

\indent Và để giảm phạm vị khi tính hàm loss ta sẽ tính tối đa hàm log của xác suất.
$$\max{\log{(\boldsymbol{y}_j^*)}}$$

\indent Nhưng vì chúng ta sẽ áp dụng gradient descent để tìm ra optimal weight nên thay vì tối đa hóa ta sẽ tìm tối thiểu hóa công thức trên như sau:
$$\min (-\log{(\boldsymbol{y}_j^*)})$$

\indent Vậy lúc này loss function của chúng ta sẽ thành:
\begin{equation}
  \mathfrak{L} = -\log{(\boldsymbol{y}_j^*)}
\end{equation}

\indent Thế phương trình $(3)$ vào phương trình $(4)$ ta thu được:
\[\begin{aligned}\mathfrak{L} &= - \log{\left ( \dfrac{\exp{(u_j)}}{\sum_{j'=1}^V \exp{(u'_j)}} \right )} \\
   &= - \left ( \log{(\exp{(u_j)})} - \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )} \right ) \\ 
   &= -\log{(\exp{(u_j)})} + \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )} \\
   &= -u_j + \log{\left ( \sum_{j'=1}^V \exp{(u'_j)} \right )}\end{aligned}\]

\paragraph{Backward propagation}
Để tối thiểu hóa loss function, chúng ta sử dụng gradient descent. Chúng ta sẽ tính gradient của loss function để cập nhật weight.\\

\indent Chúng ta có hai weight set là $\boldsymbol{W}$ và $\boldsymbol{W'}$. Trong quá trình backward propagration, chúng ta có thể tiến hành cập nhật weight cho chúng. Cụ thể như sau:
$$\boldsymbol{W} = \boldsymbol{W} - \alpha \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}}$$ 
$$\boldsymbol{W'} = \boldsymbol{W'} - \alpha \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}}$$ 

\indent Trước tiên, trong quá trình forward propagation chúng ta có các phương trình sau:
$$\boldsymbol{H} = \boldsymbol{XW}^T$$
$$\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$$
$$\mathfrak{L} = -\boldsymbol{u}_j + \log{\left ( \sum_{j'=1}^V \exp{(\boldsymbol{u'}_j)} \right )}$$

\indent Trước tiên, chúng ta sẽ tính gradient của loss function cho $\boldsymbol{W'}$. Chúng ta không thể tính gradient của loss function đối với $\boldsymbol{W'}$ trực tiếp. Cho nên chúng ta phải áp dụng \textbf{chain rule} như sau:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}_{ij}} = \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} \cdot \dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}}$$

\indent Ta có đạo hàm của $\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j}$ như sau:
\begin{equation}
  \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} = \boldsymbol{e}_j
\end{equation}

\twiceIndent với $\boldsymbol{e}_j$ là sự khác biệt giữa actual word và predicted word.

\vskip 0.5cm
$$\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$$
\indent Tiếp theo, chúng ta sẽ tính đạo hàm cho $\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}}$, và bởi vì chúng ta biết $\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$ nên:
$$\dfrac{\partial \boldsymbol{u}_j}{\partial \boldsymbol{W'}_{ij}} = \boldsymbol{H}$$

\indent Như vậy, gradient của loss function đối với $\boldsymbol{W'}$ bằng:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W'}_{ij}} = \boldsymbol{e}_j \cdot \boldsymbol{H}$$

\indent Tiếp theo, chúng ta cần tính gradient của loss function đối với $\boldsymbol{W}$, và chúng ta cũng không thể tính gradient trực tiếp nên chúng ta phải áp dụng chain rule.
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{W}_{ki}} = \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} \cdot \dfrac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{W}_{ki}}$$

\indent Để tính đạo hàm của $\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i}$, chúng ta áp dụng chain rule:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{u}_j} \cdot \dfrac{\partial \boldsymbol{u}_j}{\boldsymbol{h}_i}$$

\indent Thế phương trình $(5)$ vào phương trình trên, ta được:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \boldsymbol{e}_j \cdot \dfrac{\partial \boldsymbol{u}_j}{\boldsymbol{h}_i}$$

\indent Và vì chúng ta biết $\boldsymbol{u}_j = \boldsymbol{W'}_{ij}^T \cdot \boldsymbol{H}$, tiếp tục thế vào phương trình trên chúng ta được:
$$\dfrac{\partial \mathfrak{L}}{\partial \boldsymbol{h}_i} = \sum_{j=1}^V \boldsymbol{e}_j \cdot \boldsymbol{W'}_{ij} = \mathfrak{L}\boldsymbol{H}^T$$


\end{document}
